{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd5UXkyYeNQO",
    "colab_type": "text"
   },
   "source": [
    "# CNN VAE for SRL for DIY Self driving car\n",
    "\n",
    "In this notebook you will learn the CNN VAE(beta) model. The result model is used for state representation in reinforcement learning.\n",
    "\n",
    "First collection training data. you can use notebooks\\utility\\data_collection.ipynb \n",
    "\n",
    "Collect images of the course while driving the car on the course. Collect 1k to 10k images. Adjust the number of data collected according to the size of the course. When running the course, run in the center of the course, the side of the side line, zigzag running, etc. During the trial during reinforcement learning, you do not know how to run on the course. Collect data so that the course can be represented in the event of an error.\n",
    "\n",
    "\n",
    "## Installing TensorBoardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tensorboardX"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mount google drive\n",
    "\n",
    "You upload zip file that contain training data. The zip file copy from googledrive. \n",
    "Set zip file name to DATASET_ZIP."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b8q352pxlqy",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "DATASET_FILE = ''\n",
    "DATASET_DIR = 'dataset'\n",
    "DATASET_ZIP = os.path.join(DATASET_DIR, DATASET_FILE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tAZ4rx5eNQW",
    "colab_type": "text"
   },
   "source": [
    "## Copy from google drive\n",
    "\n",
    "Copy training data and unzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofIwQJ-J-qLM",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!rm -rf dataset_root\n",
    "!cp '/content/drive/My Drive/$DATASET_ZIP' ./\n",
    "!unzip -q $DATASET_ZIP\n",
    "\n",
    "!mkdir dataset_root\n",
    "!mv $DATASET_DIR './dataset_root'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB84g5POeNQa",
    "colab_type": "text"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RgLaYFMeNQf",
    "colab_type": "text"
   },
   "source": [
    "## Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKObTxXVxUyT",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wL75YrTqeNQi",
    "colab_type": "text"
   },
   "source": [
    "## Load GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1w9dHMDxc2t",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZXeMRs4eNQn",
    "colab_type": "text"
   },
   "source": [
    "## Load dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8kgmDhAyVe8",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "bs = 64\n",
    "dataset = datasets.ImageFolder(root='./dataset_root', transform=transforms.Compose([\n",
    "    torchvision.transforms.Resize((120, 160)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.crop((0, 40, 160, 120))),\n",
    "    transforms.ToTensor(),\n",
    "]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "len(dataset.imgs), len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJAyYP_YeNQs",
    "colab_type": "text"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxYHQhNKyX0q",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "fixed_x, _ = next(iter(dataloader))\n",
    "save_image(fixed_x, 'real_image.png')\n",
    "Image('real_image.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVLGbLF8yhy7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Define VAE Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyFNicuqyj6s",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=256):\n",
    "        return input.view(input.size(0), size, 3, 8)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=6144, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=4, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        esp = torch.randn(*mu.size()).to(device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        if self.training:\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "        else:\n",
    "            z = mu\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def loss_fn(self, images, reconst, mean, logvar):\n",
    "        KL = -0.5 * torch.sum((1 + logvar - mean.pow(2) - logvar.exp()), dim=0)\n",
    "        KL = torch.mean(KL)\n",
    "        reconstruction = F.binary_cross_entropy(reconst.view(-1,38400), images.view(-1, 38400), reduction='sum') #size_average=False)\n",
    "        return reconstruction + 5.0 * KL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2wTqO-JyncH",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Prepare Training\n",
    "\n",
    "Create VAE model and initialize optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDELfI7MUp2C",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "VARIANTS_SIZE = 32\n",
    "image_channels = fixed_x.size(1)\n",
    "vae = VAE(image_channels=image_channels, z_dim=VARIANTS_SIZE ).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "summary(vae, (3, 80, 160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n73BXe26UsiP",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hmvqsf_fw5Sp",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztKsDNhTU20H",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "QVllkoOoU5c4",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "epochs = 50\n",
    "writer = SummaryWriter()\n",
    "\n",
    "vae.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    grid = None\n",
    "    for idx, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss = vae.loss_fn(images, recon_images, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        grid = torchvision.utils.make_grid(recon_images)\n",
    "    writer.add_image('Image/reconst', grid, epoch)\n",
    "    writer.add_scalar('Loss/train',np.average(losses), epoch)\n",
    "    print(\"EPOCH: {} loss: {}\".format(epoch+1, np.average(losses)))\n",
    "\n",
    "torch.save(vae.state_dict(), 'vae.torch', _use_new_zipfile_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize latent space\n",
    "Visualizing latent space by TensorBoard.\n",
    "You can visualize latent space with TensorBoard Projector view.\n",
    "The latent spaces are auto labeled by K-means. If similar images stick together, we consider the quality of the latent space to be good.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "vae.eval()\n",
    "\n",
    "latent_spaces = None\n",
    "for idx,(images, _) in enumerate(dataloader):\n",
    "    images = images.to(device)\n",
    "    z, _, _ = vae.encode(images)\n",
    "    z = z.detach().cpu().numpy()\n",
    "    if latent_spaces is None:\n",
    "      latent_spaces = z.copy()\n",
    "    else:\n",
    "      latent_spaces = np.append(latent_spaces, z, axis=0)\n",
    "    if len(latent_spaces) > 5000:\n",
    "        break\n",
    "\n",
    "images = vae.decode(torch.Tensor(latent_spaces).to(device))\n",
    "images = F.interpolate(images, size=(40, 40), mode='bilinear', align_corners=False)\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=5, verbose=0, n_init=10)\n",
    "labels = kmeans_model.fit_predict(latent_spaces)\n",
    "\n",
    "writer.add_embedding(mat=latent_spaces, metadata=labels, label_img=images)\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Re-launch TensorBoard\n",
    "This cell do kill tensorboard process and re-launch TensorBoardX. When do not show projector tab, click reload button."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!kill $(ps | grep tensorboard | cut -f 1 -d '?')\n",
    "%tensorboard --logdir ./runs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ye2-vNLeNRf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Copy trained model file to GoogleDrive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFSRPBnAsG2g",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!cp vae.torch '/content/drive/My Drive/vae.torch'"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "VAE-CNN.ipynb のコピー",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}